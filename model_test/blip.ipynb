{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 2/2 [01:11<00:00, 35.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 캡션 생성 중...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/ephemeral/home/Frames'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 모든 프레임에 대해 캡션 생성 및 저장\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이미지 캡션 생성 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_dir\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     40\u001b[0m     frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frame_dir, frame_file)\n\u001b[1;32m     41\u001b[0m     caption \u001b[38;5;241m=\u001b[39m generate_caption(frame_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/ephemeral/home/Frames'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "\n",
    "# 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# BLIP-2 모델과 Processor 로드\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "blip_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\").to(device)\n",
    "\n",
    "# 텍스트-임베딩 모델 로드 (Sentence Transformers)\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "# 프레임 경로와 캡션 저장소\n",
    "frame_dir = \"/data/ephemeral/home/Frames\"\n",
    "caption_store = []\n",
    "\n",
    "# 이미지 캡션 생성\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, text=\"Describe the image\", return_tensors=\"pt\").to(device)\n",
    "    outputs = blip_model.generate(\n",
    "    **inputs,\n",
    "    max_length=150,           # 최대 출력 길이를 100으로 설정\n",
    "    min_length=60,            # 최소 출력 길이를 20으로 설정\n",
    "    num_beams=20,              # Beam Search 사용\n",
    "    temperature=1.3,          # 생성 다양성 제어\n",
    "    top_p=0.3,                # Top-p 샘플링 (nucleus sampling)\n",
    "    repetition_penalty=1.2    # 반복 감소\n",
    "    )\n",
    "    caption = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# 모든 프레임에 대해 캡션 생성 및 저장\n",
    "print(\"이미지 캡션 생성 중...\")\n",
    "for frame_file in sorted(os.listdir(frame_dir)):\n",
    "    frame_path = os.path.join(frame_dir, frame_file)\n",
    "    caption = generate_caption(frame_path)\n",
    "    caption_embedding = text_model.encode(caption, convert_to_tensor=True)\n",
    "    caption_store.append((frame_file, caption, caption_embedding))\n",
    "\n",
    "print(\"모든 캡션 생성 완료.\")\n",
    "\n",
    "# 검색 함수\n",
    "def search_by_text(query):\n",
    "    query_embedding = text_model.encode(query, convert_to_tensor=True)\n",
    "    similarities = []\n",
    "\n",
    "    # 모든 캡션과 유사도 계산\n",
    "    for frame_file, caption, caption_embedding in caption_store:\n",
    "        similarity = util.pytorch_cos_sim(query_embedding, caption_embedding).item()\n",
    "        similarities.append((frame_file, caption, similarity))\n",
    "\n",
    "    # 유사도 순으로 정렬\n",
    "    similarities = sorted(similarities, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # 가장 유사한 이미지 출력\n",
    "    print(\"검색 결과:\")\n",
    "    for i, (frame_file, caption, similarity) in enumerate(similarities[:5]):  # 상위 5개 출력\n",
    "        print(f\"[유사도: {similarity:.2f}] 이미지: {frame_file}, 캡션: {caption}\")\n",
    "\n",
    "    # 가장 유사한 이미지 반환\n",
    "    most_similar_frame = similarities[0][0]\n",
    "    return os.path.join(frame_dir, most_similar_frame)\n",
    "\n",
    "# 텍스트 검색 실행\n",
    "query = \"a man holding popcorn\"\n",
    "most_similar_image = search_by_text(query)\n",
    "\n",
    "# 결과 이미지 출력\n",
    "print(f\"가장 유사한 이미지: {most_similar_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.23.5\n",
      "Summary: NumPy is the fundamental package for array computing with Python.\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: BSD\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: \n",
      "Required-by: mkl-fft, mkl-random, scikit-learn, scipy, torchelastic, torchvision, transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
